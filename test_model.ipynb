{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>options</th>\n",
       "      <th>meta_info</th>\n",
       "      <th>answer_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A junior orthopaedic surgery resident is compl...</td>\n",
       "      <td>Tell the attending that he cannot fail to disc...</td>\n",
       "      <td>{'A': 'Disclose the error to the patient but l...</td>\n",
       "      <td>step1</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 67-year-old man with transitional cell carci...</td>\n",
       "      <td>Cross-linking of DNA</td>\n",
       "      <td>{'A': 'Inhibition of thymidine synthesis', 'B'...</td>\n",
       "      <td>step1</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two weeks after undergoing an emergency cardia...</td>\n",
       "      <td>Cholesterol embolization</td>\n",
       "      <td>{'A': 'Renal papillary necrosis', 'B': 'Allerg...</td>\n",
       "      <td>step2&amp;3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A 39-year-old woman is brought to the emergenc...</td>\n",
       "      <td>Lactose-fermenting, gram-negative rods forming...</td>\n",
       "      <td>{'A': 'Coagulase-positive, gram-positive cocci...</td>\n",
       "      <td>step1</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A 35-year-old man comes to the physician becau...</td>\n",
       "      <td>Ketotifen eye drops</td>\n",
       "      <td>{'A': 'Erythromycin ointment', 'B': 'Ketotifen...</td>\n",
       "      <td>step2&amp;3</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  A junior orthopaedic surgery resident is compl...   \n",
       "1  A 67-year-old man with transitional cell carci...   \n",
       "2  Two weeks after undergoing an emergency cardia...   \n",
       "3  A 39-year-old woman is brought to the emergenc...   \n",
       "4  A 35-year-old man comes to the physician becau...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Tell the attending that he cannot fail to disc...   \n",
       "1                               Cross-linking of DNA   \n",
       "2                           Cholesterol embolization   \n",
       "3  Lactose-fermenting, gram-negative rods forming...   \n",
       "4                                Ketotifen eye drops   \n",
       "\n",
       "                                             options meta_info answer_idx  \n",
       "0  {'A': 'Disclose the error to the patient but l...     step1          C  \n",
       "1  {'A': 'Inhibition of thymidine synthesis', 'B'...     step1          E  \n",
       "2  {'A': 'Renal papillary necrosis', 'B': 'Allerg...   step2&3          C  \n",
       "3  {'A': 'Coagulase-positive, gram-positive cocci...     step1          D  \n",
       "4  {'A': 'Erythromycin ointment', 'B': 'Ketotifen...   step2&3          B  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load full_dataset/data_clean/questions/US/test.jsonl\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "with open('full_dataset/data_clean/questions/US/test.jsonl') as f:\n",
    "    data = f.readlines()\n",
    "data = [json.loads(line) for line in data]\n",
    "\n",
    "# convert to dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/san/miniconda3/envs/medqa/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"ehartford/dolphin-2.2.1-mistral-7b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "#bnb_config = BitsAndBytesConfig(\n",
    "#    load_in_8bit=True,\n",
    "#)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    padding_side=\"left\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "You are an extremely skilled doctor, who will accurately respond to medical questions exactly as the user asks.\n",
    "\"\"\".strip()\n",
    "\n",
    "user_template = \"\"\"\n",
    "\n",
    "Please reason generally about answer to the following question:\n",
    "\n",
    "{question}\"\"\".strip()\n",
    "\n",
    "prompt_template = \"\"\"<|im_start|>system\n",
    " {sys_prompt} <|im_end|>\n",
    "<|im_start|>user\n",
    " {user_prompt} <|im_end|>\n",
    "<|im_start|>assistant\n",
    " \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval as le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def options_to_string(options_dict):\n",
    "    options = []\n",
    "    for k, v in options_dict.items():\n",
    "        options.append(f\"{k}: {v}\")\n",
    "    return \"\\n\".join(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      " You are an extremely skilled doctor, who will accurately respond to medical questions exactly as the user asks. <|im_end|>\n",
      "<|im_start|>user\n",
      " Please reason generally about answer to the following question:\n",
      "\n",
      "A 32-year-old man is brought by ambulance to the emergency room after being involved in a head-on motor vehicle collision at high speed. The patient was found unconscious by the paramedics and regained consciousness briefly during the ambulance ride. Upon arrival at the hospital, the patient’s vitals show: pulse 110/min, respiratory rate 12/min, blood pressure 100/70 mm Hg, and oxygen saturation of 96%. Physical examination reveals an unresponsive man with multiple bruises across the chest and along the upper arms with a laceration on the forehead. His is unresponsive to verbal commands and physical touch. His GCS is 6/15. The right pupil is fixed and dilated. An urgent noncontrast CT of the head is performed and shown in the image. The patient is prepared for emergency neurosurgery. Which of the following anesthesia medications would be the best option for this patient?\n",
      "A: Propofol\n",
      "B: Halothane\n",
      "C: Midazolam\n",
      "D: Nitrous oxide\n",
      "E: Sevoflurane <|im_end|>\n",
      "<|im_start|>assistant\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def make_prompt(dataset_row):\n",
    "    question = dataset_row[\"question\"]\n",
    "    options = dataset_row[\"options\"]\n",
    "\n",
    "    # print(dataset_row[\"answer_idx\"])\n",
    "\n",
    "    options = options_to_string(options)\n",
    "\n",
    "    question = f\"{question}\\n{options}\"\n",
    "\n",
    "    prompt = prompt_template.format(\n",
    "        sys_prompt=sys_prompt,\n",
    "        user_prompt=user_template.format(question=question)\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "sample = df.sample(1)\n",
    "sample_prompt = make_prompt(sample.iloc[0])\n",
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/160 [00:00<?, ?it/s]/home/san/miniconda3/envs/medqa/lib/python3.10/site-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/160 [01:37<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/san/projects/MED-QA/test_model.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m eval_tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     batch_prompts,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# padding_side=\"left\",\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# Generate the model output for the batch of prompts\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m batch_output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs, max_length\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Decode the output and append it to the outputs list\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m batch_output_decoded \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     eval_tokenizer\u001b[39m.\u001b[39mdecode(output, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m batch_output\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bsan_linux_compute/home/san/projects/MED-QA/test_model.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/transformers/generation/utils.py:1717\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1701\u001b[0m         input_ids,\n\u001b[1;32m   1702\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1715\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1716\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1717\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1718\u001b[0m         input_ids,\n\u001b[1;32m   1719\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1720\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1721\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1722\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1723\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1724\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1725\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1726\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1727\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1728\u001b[0m     )\n\u001b[1;32m   1730\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1731\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/transformers/generation/utils.py:2578\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2575\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2577\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2578\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2579\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2580\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2581\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2582\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2583\u001b[0m )\n\u001b[1;32m   2585\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2586\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1007\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1006\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1007\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1008\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1009\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1010\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1011\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1012\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1013\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1014\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1015\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1016\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1017\u001b[0m )\n\u001b[1;32m   1019\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1020\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:912\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    910\u001b[0m         all_self_attns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (layer_outputs[\u001b[39m1\u001b[39m],)\n\u001b[0;32m--> 912\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm(hidden_states)\n\u001b[1;32m    914\u001b[0m \u001b[39m# add hidden states from the last decoder layer\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/medqa/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:83\u001b[0m, in \u001b[0;36mMistralRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m     82\u001b[0m     input_dtype \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mdtype\n\u001b[0;32m---> 83\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m     84\u001b[0m     variance \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariance_epsilon)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompts = []\n",
    "# reasons = []\n",
    "outputs = []\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    prompt = make_prompt(row)\n",
    "    prompts.append(prompt)\n",
    "    model_input = eval_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    output = eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True)\n",
    "    outputs.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/san/miniconda3/envs/medqa/lib/python3.10/site-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> system\n",
      " You are an extremely skilled doctor, who will accurately respond to medical questions exactly as the user asks.  \n",
      "<|im_start|> user\n",
      " Please reason the answer to the following question:\n",
      "\n",
      "A 38-year-old nursing home worker presents to the clinic with complaints of fever, loss of appetite, fatigue, and productive cough for the past couple of months. His fever is low-grade and sputum is often blood-tinged. He has lost 6.8 kg (15.0 lb) during this period and complains of profound night sweats. A plain radiograph of the patient’s chest shows consolidation in the apical part of the right lung. Baseline investigations show the following:\n",
      "Complete blood count\n",
      "Hemoglobin 11 g/dL\n",
      "White blood cells  \n",
      "Total count 16,000/mm3\n",
      "Differential count  \n",
      "Neutrophils 35%\n",
      "Lymphocytes 54%\n",
      "Eosinophils 11%\n",
      "Erythrocyte sedimentation rate 84 mm\n",
      "The physician suspects that the patient is suffering from a chronic lung infection. Which of the following statements best describes the type of lung inflammation in this patient?\n",
      "A: There are small granulomas with few epithelioid cells along with fibrosis.\n",
      "B: It has a granuloma with Anitchov cells around a core of fibrinoid collagen necrosis.\n",
      "C: It consists of a largely circumscribed granuloma with epithelioid cells with Langhans cells.\n",
      "D: An ill-defined granuloma with macrophages and epithelioid cells is seen in this type of inflammation.\n",
      "E: This type of granulomatous inflammation is also seen in histoplasmosis.  \n",
      "<|im_start|> assistant\n",
      " \n",
      "The type of lung inflammation in this patient is best described by option D: An ill-defined granuloma with macrophages and epithelioid cells is seen in this type of inflammation. This type of granulomatous inflammation is also seen in histoplasmosis. \n",
      "\n",
      "The other options do not accurately describe the type of lung inflammation in this patient. Option A refers to sarcoidosis, option B refers to Wegener's granulomatosis, and option C refers to tuberculosis. These conditions can present with similar symptoms, but the patient's history and chest radiograph findings suggest a different diagnosis.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_input = eval_tokenizer(sample_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_path = (\n",
    "    \"train_outputs/medqa_mistral_dolphin_2_2_1-medqa_mixed_qa_train_20k/checkpoint-750\"\n",
    ")\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(model, ft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> system\n",
      " You are an extremely skilled doctor, who will accurately respond to medical questions exactly as the user asks.  \n",
      "<|im_start|> user\n",
      " Please reason the answer to the following question:\n",
      "\n",
      "A 38-year-old nursing home worker presents to the clinic with complaints of fever, loss of appetite, fatigue, and productive cough for the past couple of months. His fever is low-grade and sputum is often blood-tinged. He has lost 6.8 kg (15.0 lb) during this period and complains of profound night sweats. A plain radiograph of the patient’s chest shows consolidation in the apical part of the right lung. Baseline investigations show the following:\n",
      "Complete blood count\n",
      "Hemoglobin 11 g/dL\n",
      "White blood cells  \n",
      "Total count 16,000/mm3\n",
      "Differential count  \n",
      "Neutrophils 35%\n",
      "Lymphocytes 54%\n",
      "Eosinophils 11%\n",
      "Erythrocyte sedimentation rate 84 mm\n",
      "The physician suspects that the patient is suffering from a chronic lung infection. Which of the following statements best describes the type of lung inflammation in this patient?\n",
      "A: There are small granulomas with few epithelioid cells along with fibrosis.\n",
      "B: It has a granuloma with Anitchov cells around a core of fibrinoid collagen necrosis.\n",
      "C: It consists of a largely circumscribed granuloma with epithelioid cells with Langhans cells.\n",
      "D: An ill-defined granuloma with macrophages and epithelioid cells is seen in this type of inflammation.\n",
      "E: This type of granulomatous inflammation is also seen in histoplasmosis.  \n",
      "<|im_start|> assistant\n",
      " \n",
      "The type of lung inflammation in this patient is best described by option D: An ill-defined granuloma with macrophages and epithelioid cells is seen in this type of inflammation. This is because the patient's symptoms and chest radiograph findings are consistent with a chronic lung infection, and the presence of an ill-defined granuloma with macrophages and epithelioid cells is characteristic of this type of inflammation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    reason = eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True)\n",
    "    print(reason)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_prompt = f\"\"\"\n",
    "<|im_start|>user\n",
    " Please reply only with the option A, B, ... and nothing else <|im_end|>\n",
    "<|im_start|>assistant\n",
    " The letter option for the correct answer is: \"\"\"\n",
    "\n",
    "\n",
    "prompt = reason + follow_up_prompt\n",
    "\n",
    "model_input = eval_tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:32000 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "answer = eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=1024)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>  system\n",
      " You are an extremely skilled doctor, who will accurately respond to medical questions exactly as the user asks.  \n",
      "<|im_start|>  user\n",
      " Please reason the answer to the following question:\n",
      "\n",
      "A 38-year-old nursing home worker presents to the clinic with complaints of fever, loss of appetite, fatigue, and productive cough for the past couple of months. His fever is low-grade and sputum is often blood-tinged. He has lost 6.8 kg (15.0 lb) during this period and complains of profound night sweats. A plain radiograph of the patient’s chest shows consolidation in the apical part of the right lung. Baseline investigations show the following:\n",
      "Complete blood count\n",
      "Hemoglobin 11 g/dL\n",
      "White blood cells  \n",
      "Total count 16,000/mm3\n",
      "Differential count  \n",
      "Neutrophils 35%\n",
      "Lymphocytes 54%\n",
      "Eosinophils 11%\n",
      "Erythrocyte sedimentation rate 84 mm\n",
      "The physician suspects that the patient is suffering from a chronic lung infection. Which of the following statements best describes the type of lung inflammation in this patient?\n",
      "A: There are small granulomas with few epithelioid cells along with fibrosis.\n",
      "B: It has a granuloma with Anitchov cells around a core of fibrinoid collagen necrosis.\n",
      "C: It consists of a largely circumscribed granuloma with epithelioid cells with Langhans cells.\n",
      "D: An ill-defined granuloma with macrophages and epithelioid cells is seen in this type of inflammation.\n",
      "E: This type of granulomatous inflammation is also seen in histoplasmosis.  \n",
      "<|im_start|>  assistant\n",
      " \n",
      "The type of lung inflammation in this patient is best described by option D: An ill-defined granuloma with macrophages and epithelioid cells is seen in this type of inflammation. This is because the patient's symptoms and chest radiograph findings are consistent with a chronic lung infection, and the presence of an ill-defined granuloma with macrophages and epithelioid cells is characteristic of this type of inflammation.\n",
      "<|im_start|> user\n",
      " Please reply only with the option A, B, ... and nothing else  \n",
      "<|im_start|> assistant\n",
      " The letter option for the correct answer is: \n",
      "\n",
      "D\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
